{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize a Segmentation Model and Show Live Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On Windows, try to find the directory that contains x64 cl.exe and add it to the PATH to enable PyTorch\n",
    "to find the required C++ tools. This code assumes that Visual Studio is installed in the default\n",
    "directory. If you have a different C++ compiler, please add the correct path to os.environ[\"PATH\"]\n",
    "directly. Note that the C++ Redistributable is not enough to run this notebook.\n",
    "\n",
    "Adding the path to os.environ[\"LIB\"] is not always required - it depends on the system's configuration\n",
    "\"\"\"\n",
    "\n",
    "from re import search\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    import distutils.command.build_ext\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    if sys.getwindowsversion().build >= 20000:\n",
    "        search_path = \"**/Hostx64/x64/cl.exe\"    \n",
    "    else:\n",
    "        search_path = \"**/Hostx86/x64/cl.exe\"\n",
    "\n",
    "    VS_INSTALL_DIR_2019 = r\"C:/Program Files (x86)/Microsoft Visual Studio\"\n",
    "    VS_INSTALL_DIR_2022 = r\"C:/Program Files/Microsoft Visual Studio\"\n",
    "\n",
    "    cl_paths_2019 = sorted(list(Path(VS_INSTALL_DIR_2019).glob(search_path)))\n",
    "    cl_paths_2022 = sorted(list(Path(VS_INSTALL_DIR_2022).glob(search_path)))\n",
    "    cl_paths = cl_paths_2019 + cl_paths_2022\n",
    "\n",
    "    if len(cl_paths) == 0:\n",
    "        raise ValueError(\n",
    "            \"\"\"\n",
    "            Cannnot find Visual Studio. This notebook requires an x64 C++ compiler. If you installed a\n",
    "            C++ compiler, please add the directory that contains cl.exe to `os.environ[PATH]`\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        cl_path = cl_paths[-1]\n",
    "        vs_dir = str(cl_path.parent)\n",
    "        os.environ[\"PATH\"] += f\"{os.pathsep}{vs_dir}\"\n",
    "        d =  distutils.core.Distribution()\n",
    "        b = distutils.command.build_ext.build_ext(d)\n",
    "        b.finalize_options()\n",
    "        os.environ[\"LIB\"] = os.pathsep.join(b.library_dirs)\n",
    "        print(f\"Added {vs_dir} to PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tkinter.ttk import Notebook\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.transforms import LoadImage\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "from openvino.inference_engine import IECore\n",
    "from openvino.runtime import Core\n",
    "from torch.jit import TracerWarning\n",
    "from torchmetrics import F1\n",
    "\n",
    "set_log_level(logging.ERROR)\n",
    "\n",
    "sys.path.append(\"../utils/\")\n",
    "from models.custom_segmentation import SegmentationModel\n",
    "from notebook_utils import NotebookAlert, benchmark_model, download_file, show_live_inference\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    from nncf import NNCFConfig\n",
    "    from nncf.torch.initialization import register_default_init_args\n",
    "    from nncf.torch.model_creation import create_compressed_model\n",
    "except subprocess.CalledProcessError:\n",
    "    message = \"WARNING: Running this notebook requires an x64 C++ compiler\"\n",
    "    NotebookAlert(message=message, alert_class=\"warning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = Path(\"kits19_frames_1\")\n",
    "# Uncomment the line below to use the full dataset, as prepared in the data preparation notebook\n",
    "# BASEDIR = Path(\"~/kits19/kits19_frames\").expanduser()\n",
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_url = \"https://github.com/helena-intel/openvino_notebooks/raw/110-nncf/notebooks/110-ct-segmentation-quantize/pretrained_model/unet_kits19_state_dict.pth\"\n",
    "state_dict_file = download_file(state_dict_url, directory=\"pretrained_model\")\n",
    "state_dict = torch.load(state_dict_file, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace(\"_model.\", \"\")\n",
    "    new_state_dict[new_key] = v\n",
    "new_state_dict.pop(\"loss_function.pos_weight\")\n",
    "\n",
    "model = monai.networks.nets.BasicUNet(spatial_dims=2, in_channels=1, out_channels=1).eval()\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 512, 512)\n",
    "fp32_onnx_path = MODEL_DIR / \"unet_kits19_fp32.onnx\"\n",
    "torch.onnx.export(model, dummy_input, fp32_onnx_path)\n",
    "!mo --input_model \"$fp32_onnx_path\" --output_dir $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CT-scan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CT scan case number. For example: 2 for data from the case_00002 directory\n",
    "# Currently only 117 is supported\n",
    "CASE = 117\n",
    "if not (BASEDIR / f\"case_{CASE:05d}\").exists():\n",
    "    BASEDIR.mkdir(exist_ok=True)\n",
    "    filename = download_file(\n",
    "        f\"https://storage.openvinotoolkit.org/data/test_data/openvino_notebooks/kits19/case_{CASE:05d}.zip\"\n",
    "    )\n",
    "\n",
    "    # debug\n",
    "    print(filename)\n",
    "\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=BASEDIR)\n",
    "        print(f\"Downloaded and extracted data for case_{CASE:05d}\")\n",
    "else:\n",
    "    print(F\"Data for case_{CASE:05d} exists\")\n",
    "\n",
    "# remove zip file\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_and_flip(image):\n",
    "    \"\"\"Rotate `image` by 90 degrees and flip horizontally\"\"\"\n",
    "    return cv2.flip(cv2.rotate(image, rotateCode=cv2.ROTATE_90_CLOCKWISE), flipCode=1)\n",
    "\n",
    "class KitsDataset:\n",
    "    def __init__(self, basedir:str):\n",
    "        \"\"\"\n",
    "        Dataset class for prepared kits19 data, for binary segmentation (background/kidney)\n",
    "        Source data should exist in basedir, in subdirectories case_0000 until case_00210,\n",
    "        with each subdirectory containing directories imaging_frames, with jpg images, and\n",
    "        segmentation_frames with segmentation masks as png files.\n",
    "        See https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize/data-preparation-ct-scan.ipynb\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans\n",
    "        \"\"\"\n",
    "\n",
    "        masks = sorted(BASEDIR.glob(\"case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        self.basedir = basedir\n",
    "        self.dataset = masks\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            Created dataset with {len(self.dataset)} items.\n",
    "            Base directory for data: {basedir}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset ath the specified index.\n",
    "\n",
    "        :return : (image, segmentation_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        mask_path = self.dataset[index]\n",
    "        image_path = str(mask_path.with_suffix(\".jpg\")).replace(\n",
    "            \"segmentation_frames\", \"imaging_frames\"\n",
    "        )\n",
    "\n",
    "        # Load images with MONAI's LoadImage to match data loading in training notebook\n",
    "        mask = LoadImage(image_only=True, dtype=np.uint8)(str(mask_path))\n",
    "        img = LoadImage(image_only=True, dtype=np.float32)(str(image_path))\n",
    "\n",
    "        if img.shape[:2] != (512, 512):\n",
    "            img = cv2.resize(img.astype(np.uint8), (512, 512)).astype(np.float32)\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        input_image = np.expand_dims(img, axis=0)\n",
    "        return input_image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KitsDataset(BASEDIR)\n",
    "# Find a slice that contains kindney annotations\n",
    "# item[0] is the annotation: (id, annotation_data)\n",
    "image_data, mask = next(item for item in dataset if np.count_nonzero(item[1]) > 5000)\n",
    "\n",
    "# Remove extra image dimenstion and rotate and flip the image for visualization\n",
    "image = rotate_and_flip(image_data.squeeze())\n",
    "\n",
    "# The data loader returns annotations as (index, mask) and mask in shape (H, W)\n",
    "mask = rotate_and_flip(mask)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image, cmap=\"gray\")\n",
    "ax[1].imshow(mask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(model: torch.nn.Module, dataset: KitsDataset):\n",
    "    \"\"\"\n",
    "    Compute binary F1 score of `model` on `dataset`\n",
    "    F1 score metric is provided by the torchmetirc library\n",
    "    `model` is expected to be a binary segmentation model, images in the \n",
    "    dataet are expected in (N, C, H, W) format where N==C==1\n",
    "    \"\"\"\n",
    "\n",
    "    metric = F1(ignore_index=0)\n",
    "    with torch.no_grad():\n",
    "        for image, target in dataset:\n",
    "            input_image = torch.as_tensor(image).unsqueeze(0)\n",
    "            output = model(input_image)\n",
    "            label = torch.as_tensor(target.squeeze()).long()\n",
    "            prediction = torch.sigmoid(output.squeeze()).round().long()\n",
    "            metric.update(label.flatten(), prediction.flatten())\n",
    "\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_f1 = compute_f1(model, dataset)\n",
    "print(f\"FP32 F1: {fp32_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NNCF uses the model loaded at the beginning of this notebook. If after quantizing the model,\n",
    "you want to quantize with a different config, reload the model by uncommenting the next tow lines\n",
    "\"\"\"\n",
    "\n",
    "# model = monai.networks.nets.BasicUNet(spatial_dims=2, in_channels=1, out_channels=1).eval()\n",
    "# model.load_state_dict(new_state_dict)\n",
    "\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 1, 512, 512]},\n",
    "    \"target_device\": \"CPU\",\n",
    "    \"compression\":{\n",
    "        \"algorithm\": \"quantization\",\n",
    "        \"preset\": \"performance\",\n",
    "        \"ignored_scopes\": [\"{re}.*LeakyReLU\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ここから先のプログラムはデスクトップ推奨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "nncf_config = register_default_init_args(nncf_config, data_loader)\n",
    "compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_onnx_path = MODEL_DIR / \"unet_kits19_int8.onnx\"\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "compression_ctrl.export_model(str(int8_onnx_path))\n",
    "print(f\"INT8 ONNX model exported to {int8_onnx_path}\")\n",
    "\n",
    "!mo --input_model \"$int8_onnx_path\" --input_shape \"[1, 1 512, 512]\" --output_dir \"$MODEL_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare FP32 and INT8 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_ir_path = Path(fp32_onnx_path).with_suffix(\".xml\")\n",
    "int8_ir_path = Path(int8_onnx_path).with_suffix(\".xml\")\n",
    "\n",
    "original_model_size = fp32_ir_path.with_suffix(\".bin\").stat().st_size / 1024\n",
    "quantized_model_size = int8_ir_path.with_suffix(\".bin\").stat().st_size / 1024\n",
    "\n",
    "print(f\"FP32 model size: {original_model_size:.2f}KB\")\n",
    "print(f\"INT8 model size: {quantized_model_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_f1 = compute_f1(compressed_model, dataset)\n",
    "\n",
    "print(f\"FP32 F1: {fp32_f1:.3f}\")\n",
    "print(f\"INT8 F1: {int8_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance of the Original and Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters and docstring for `benchmark_model`\n",
    "benchmark_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP32 model\n",
    "benchmark_model(model_path=fp32_ir_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark INT8 model\n",
    "benchmark_model(model_path=int8_ir_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Compare Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "num_images = 4\n",
    "colormap = \"gray\"\n",
    "\n",
    "# Loat FP32 and INT8 models\n",
    "core = Core()\n",
    "fp_model = core.read_model(fp32_ir_path)\n",
    "int8_model = core.read_model(int8_ir_path)\n",
    "compiled_model_fp = core.compile_model(fp_model, device_name=\"CPU\")\n",
    "compiled_model_int8 = core.compile_model(int8_model, device_name=\"CPU\")\n",
    "output_layer_fp = compiled_model_fp.output(0)\n",
    "output_layer_int8 = compiled_model_int8.output(0)\n",
    "\n",
    "# Create subset of dataset\n",
    "background_slices = (item for item in dataset if np.count_nonzero(item[1]) == 0)\n",
    "kindney_slices = (item for item in dataset if np.count_nonzero(item[1]) > 50)\n",
    "data_subset = random.sample(list(background_slices), 2) + random.sample(list(kindney_slices), 2)\n",
    "\n",
    "# Set seed to current time. To reproduce specific results, copy the printed\n",
    "# and manually set `read` to that value\n",
    "seed = int(time.time())\n",
    "random.seed(seed)\n",
    "print(f\"Visualizing results with seed {seed}\")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_images, ncols=4, figsize=(24, num_images * 4))\n",
    "for i, (image, mask) in enumerate(data_subset):\n",
    "    display_image = rotate_and_flip(image.squeeze())\n",
    "    target_mask = rotate_and_flip(mask).astype(np.uint8)\n",
    "\n",
    "    # Add batch dimension to image and do inference on FP and INT8 models\n",
    "    input_image = np.expand_dims(image, 0)\n",
    "    res_fp = compiled_model_fp([input_image])\n",
    "    res_int8 = compiled_model_int8([input_image])\n",
    "\n",
    "    # Process inference outputs and convert to binary segmentation masks\n",
    "    result_mask_fp = sigmoid(res_fp[output_layer_fp]).squeeze().round().astype(np.uint8)\n",
    "    result_mask_int8 = sigmoid(res_int8[output_layer_int8]).squeeze().round().astype(np.uint8)\n",
    "    result_mask_fp = rotate_and_flip(result_mask_fp)\n",
    "    result_mask_int8 = rotate_and_flip(result_mask_int8)\n",
    "\n",
    "    # Display images, annotations, FP32 result and INT8 result\n",
    "    ax[i, 0].imshow(display_image, cmap=colormap)\n",
    "    ax[i, 1].imshow(target_mask, cmap=colormap)\n",
    "    ax[i, 2].imshow(result_mask_fp, cmap=colormap)\n",
    "    ax[i, 3].imshow(result_mask_int8, cmap=colormap)\n",
    "    ax[i, 2].set_title(\"Prediction on FP32 model\")\n",
    "    ax[i, 3].set_title(\"Prediction on INT8 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and list of image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE = 117\n",
    "\n",
    "# The live inference function uses the OpenVINO Runtime API which is compatible with\n",
    "# OpenVINO LTS release 2021.4\n",
    "\n",
    "ie = IECore()\n",
    "segmentation_model = SegmentationModel(\n",
    "    ie=ie, model_path=Path(int8_ir_path), sigmoid=True, rotate_and_flip=True\n",
    ")\n",
    "\n",
    "case_path = BASEDIR / f\"case_{CASE:05d}\"\n",
    "image_paths = sorted(case_path.glob(\"imaging_frames/*jpg\"))\n",
    "print(f\"{case_path.name}, {len(image_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible options for device include CPU GPU AUTO MULTI:CPU,GPU\n",
    "device = \"CPU\"\n",
    "reader = LoadImage(image_only=True, dtype=np.uint8)\n",
    "show_live_inference(\n",
    "    ie=ie, image_paths=image_paths, model=segmentation_model, device=device, reader=reader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0799a591d602a7578f1ccf5f8e7829399f6cceabacecc6a7299b85400bce773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
