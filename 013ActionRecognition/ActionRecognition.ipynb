{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.runtime import Core\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = \"model\"\n",
    "model_name = \"action-recognition-0001\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "model_path_decoder = (\n",
    "    f\"./model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    ")\n",
    "\n",
    "model_path_encoder = (\n",
    "    f\"./model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(model_path_decoder) or not os.path.exists(model_path_encoder):\n",
    "    download_command = f\"omz_downloader --name {model_name} --precision {precision} --output_dir {base_model_dir}\" \n",
    "\n",
    "    # ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"data/kinetics.txt\"\n",
    "\n",
    "with open(labels) as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "\n",
    "print(labels[0:9], np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models\n",
    "\n",
    "### Model Initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_path) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from a file, load the model on CPU\n",
    "    and get input and output names of nodes\n",
    "\n",
    "    :param: model: architecture path *.xml\n",
    "    :returns:\n",
    "        compiled_model: Compiled model\n",
    "        input_key: Input node for model\n",
    "        output_key: Output node for model\n",
    "    \"\"\"\n",
    "    ie_core = Core()\n",
    "\n",
    "    model = ie_core.read_model(model_path)\n",
    "    compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "    input_keys = compiled_model.input(0)\n",
    "    output_keys = compiled_model.output(0)\n",
    "\n",
    "    # debug\n",
    "    print(f\"input_keys: {input_keys}\")\n",
    "    print(f\"output_keys: {output_keys}\")\n",
    "\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_key_en, output_keys_en, compiled_model_en = model_init(model_path_encoder)\n",
    "\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(model_path_decoder)\n",
    "\n",
    "height_en, width_en = list(input_key_en.shape)[2:]\n",
    "frames2decode = list(input_key_de.shape)[0:][1]\n",
    "\n",
    "# debug\n",
    "print(f\"frames2decode: {frames2decode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center crop squared the original frame to standardize the input image to the encoder model\n",
    "\n",
    "    :param frame: input frame\n",
    "    :returns: center-crop-squared frame\n",
    "    \"\"\"\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    min_dim = min(height, width)\n",
    "    start_x = int((width - min_dim) / 2.0)\n",
    "    start_y = int((height - min_dim) / 2.0)\n",
    "    roi = [start_y, (start_y + min_dim), start_x, (start_x + min_dim)]\n",
    "    return frame[start_y : (start_y + min_dim), start_x : (start_x + min_dim), ...], roi\n",
    "\n",
    "\n",
    "def adaptive_resize(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The frame going to be resized to have a height of size or a width of size\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized frame, np.array type\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    scale = size / min(h, w)\n",
    "    w_scaled, h_scaled = int(w * scale), int(h * scale)\n",
    "    if w_scaled == w and h_scaled == h:\n",
    "        return frame\n",
    "    return cv2.resize(frame, (w_scaled, h_scaled))\n",
    "\n",
    "\n",
    "def decode_output(probs: np.ndarray, labels: np.ndarray, top_k: int=3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param probs: confidence vector for 400 actions\n",
    "    :param labels: list of actions\n",
    "    :param top_k: The k most probable positions in the list of labels\n",
    "    :returns: decoded_labels: the k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence fot the k most probable actions\n",
    "    \"\"\"\n",
    "\n",
    "    top_ind = np.argsort(-1 * probs)[:top_k]\n",
    "    out_label = np.array(labels)[top_ind.astype(int)]\n",
    "    decoded_labels = [out_label[0][0], out_label[0][1], out_label[0][2]]\n",
    "    top_probs = np.array(probs)[0][top_ind.astype(int)]\n",
    "    decoded_top_probs = [top_probs[0][0], top_probs[0][1], top_probs[0][2]]\n",
    "\n",
    "    # debug\n",
    "    print(f\"top_ind: {top_ind}\")\n",
    "    print(f\"decoded_labels: {decoded_labels}\")\n",
    "    print(f\"decoded_top_probs: {decoded_top_probs}\")\n",
    "\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "def rec_frame_display(frame: np.ndarray, roi) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw a rec frame over actual frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param roi: Region of interest, image section processed by the Encoder\n",
    "    :returns: frame with drawed shape\n",
    "    \"\"\"\n",
    "    # debug\n",
    "    print(roi[2])\n",
    "    print(roi[0])\n",
    "\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_COMPLEX\n",
    "    org = (roi[2] + 3, roi[1] - 3)\n",
    "    org2 = (roi[2] + 2, roi[1] - 2)\n",
    "    FONT_SIZE = 0.5\n",
    "    FONT_COLOR = (0, 200, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    cv2.putText(frame, \"ROI\", org2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, \"ROI\", org, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "    return frame\n",
    "\n",
    "def display_text_fnc(frame: np.ndarray, display_text: str, index: int):\n",
    "    \"\"\"\n",
    "    Include a text on the analyzed frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param display_text: text to add on the frame\n",
    "    :param index: index line dor adding text\n",
    "    \"\"\"\n",
    "\n",
    "    FONT_COLOR = (255, 255, 255)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_COMPLEX\n",
    "    FONT_SIZE = 0.7\n",
    "    TEXT_VERTICAL_INTERVAL = 25\n",
    "    TEXT_LEFT_MARGIN = 15\n",
    "    (processed, roi) = center_crop(frame)\n",
    "\n",
    "    frame = rec_frame_display(frame, roi)\n",
    "    \n",
    "    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (index + 1))\n",
    "    text_loc2 = (TEXT_LEFT_MARGIN + 1, TEXT_VERTICAL_INTERVAL * (index + 1) + 1)\n",
    "    cv2.putText(frame, display_text, text_loc2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preparing frame before Encoder.\n",
    "    The image should be scaled to its shortest dimension at \"size\"\n",
    "    and cropped, centered, and squared so that oth width and height\n",
    "    have lengths \"size\". The frame must be transposed from\n",
    "    HWC to CHW\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns : resized and cropped frame\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed = adaptive_resize(frame, size)\n",
    "    (preprocessed, roi) = center_crop(preprocessed)\n",
    "    preprocessed = preprocessed.transpose((2, 0, 1))[None, ]\n",
    "\n",
    "    return preprocessed, roi\n",
    "\n",
    "\n",
    "def encoder(preprocessed: np.ndarray, compiled_model: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Encoder Inference per frame. This funcition calls the network preiously\n",
    "    configured fro the encoder model (compiled model), extracts the data\n",
    "    from the output node, and appends it in an array to be used by the decoder\n",
    "\n",
    "    :param: preprocessed: preprocessing frame\n",
    "    :param: compiled_model: Encoder model network\n",
    "    :returns: encoder_output: embedding layer that is append with each arriving frame\n",
    "    \"\"\"\n",
    "\n",
    "    output_keys_en = compiled_model.output(0)\n",
    "\n",
    "    infer_result_encoder = compiled_model([preprocessed])[output_keys_en]\n",
    "    return infer_result_encoder\n",
    "\n",
    "\n",
    "def decoder(encoder_output: list, compiled_model_de: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames this function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to atch with the decoder input size.\n",
    "    Calls the network previously configured fot the decoder model (compiled_model_de), etracts]\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decoders top probabilities into corresponding label names.\n",
    "\n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: compiled_model_de: Decoder model network\n",
    "    :returns: decoded labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "\n",
    "    decoder_input = decoder_input.transpose((2, 0, 1, 3))\n",
    "    decoder_input = np.squeeze(decoder_input, axis=3)\n",
    "    output_key_de = compiled_model_de.output(0)\n",
    "\n",
    "    result_de = compiled_model_de([decoder_input])[output_key_de]\n",
    "    probs = softmax(result_de - np.max(result_de))\n",
    "\n",
    "    decoded_labels, decoded_top_probs = decode_output(probs, labels, top_k=3)\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    X: np.array, axis=None\n",
    "    \"\"\"\n",
    "\n",
    "    exp = np.exp(x)\n",
    "    print(exp)\n",
    "    return exp / np.sum(exp, axis=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(\n",
    "    source: str = \"0\",\n",
    "    flip: bool = True,\n",
    "    use_popup: bool = False,\n",
    "    compiled_model_en: CompiledModel = compiled_model_en,\n",
    "    compiled_model_de: CompiledModel = compiled_model_de,\n",
    "    skip_first_frames: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use the \"source\" webcam or video file to run the complete pipeline for action-recognition problem\n",
    "    1. Create a video player to play with target fps\n",
    "    2. Prepare a set of frames to be encoded-decoded\n",
    "    3. Preprocess frame before Encoder\n",
    "    4. Encoder Inference per frame\n",
    "    5. Decoder inference per set of frames\n",
    "    6. Visualize the results\n",
    "\n",
    "    :param: source: webcam \"0\" or video path\n",
    "    :param: flip: to be used by VideoPlayer function for flipping capture image\n",
    "    :param: use_popup: False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "    :param: skip_first_frames: Number of frames to skip at the beginning of the video.\n",
    "    :returns: display video over the notebook or in a popup window\n",
    "    \"\"\"\n",
    "\n",
    "    size = height_en\n",
    "    sample_duration = frames2decode\n",
    "    fps = 30\n",
    "    player = None\n",
    "    try:\n",
    "        player = utils.VideoPlayer(source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "        \n",
    "        processing_times = collections.deque()\n",
    "        processing_time = 0\n",
    "        encoder_output = []\n",
    "        decoded_labels = [0, 0, 0]\n",
    "        decoded_top_probs = [0, 0, 0]\n",
    "        counter = 0\n",
    "\n",
    "        text_inference_template = \"Infer Time:{Time:.1f}ms, {fps:.1f}FPS\"\n",
    "        text_template = \"{label}, {conf:.2f} %\"\n",
    "\n",
    "        while True:\n",
    "            counter = counter + 1\n",
    "\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            \n",
    "            scale = 1280 / max(frame.shape)\n",
    "\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            if counter % 2 == 0:\n",
    "                (preprocessed, _) = preprocessing(frame, size)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "\n",
    "                if len(encoder_output) == sample_duration:\n",
    "                    decoded_labels, decoded_top_probs = decoder(encoder_output, compiled_model_de)\n",
    "                    encoder_output = []\n",
    "\n",
    "                stop_time = time.time()\n",
    "\n",
    "                processing_times.append(stop_time - start_time)\n",
    "\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "\n",
    "            for i in range(0, 3):\n",
    "                display_text = text_template.format(\n",
    "                    label=decoded_labels[i],\n",
    "                    conf=decoded_top_probs[i] * 100\n",
    "                )\n",
    "                display_text_fnc(frame, display_text, i)\n",
    "\n",
    "            display_text = text_inference_template.format(Time=processing_time, fps=fps)\n",
    "            display_text_fnc(frame, display_text, 3)\n",
    "\n",
    "            if use_popup:\n",
    "                cv2.imshow(title, frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "                i = display.Image(data=encoded_img)\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = \"https://archive.org/serve/ISSVideoResourceLifeOnStation720p/ISS%20Video%20Resource_LifeOnStation_720p.mp4\"\n",
    "run_action_recognition(source=video_file, flip=False, use_popup=False, skip_first_frames=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run action recognition using a webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_action_recognition(source=0, flip=False, use_popup=False, skip_first_frames=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0799a591d602a7578f1ccf5f8e7829399f6cceabacecc6a7299b85400bce773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
