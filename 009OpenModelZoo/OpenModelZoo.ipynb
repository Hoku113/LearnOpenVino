{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Open Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you forgot Open Model Zoo Tools, please click [here](https://docs.openvino.ai/latest/notebooks/104-model-tools-with-output.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"resnet-50-pytorch\"\n",
    "model_name = \"mobilenet-v2-pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import DeviceNotFoundAlert, NotebookAlert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = Path(\"~/open_model_zoo_models\").expanduser()\n",
    "omz_cache_dir = Path(\"~/open_model_zoo_cache\").expanduser()\n",
    "precision = \"FP16\"\n",
    "\n",
    "# Check if an iGPU is available on this system to use with Benchmark App.\n",
    "ie = Core()\n",
    "gpu_available = \"GPU\" in ie.available_devices\n",
    "\n",
    "print(f\"\"\"\n",
    "    base_model_dir: {base_model_dir}\n",
    "    omz_cache_dir: {omz_cache_dir}\n",
    "    gpu_available: {gpu_available}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Model from OMZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the next line to show help in omz_downloader which explains the command-line options.\n",
    "\n",
    "# !omz_downloader --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = (\n",
    "    f\"omz_downloader --name {model_name} --output_dir {base_model_dir} --cache_dir {omz_cache_dir}\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name} ...\"))\n",
    "\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a Model to OpenVINO IR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {base_model_dir} --output_dir {base_model_dir}\"\n",
    "\n",
    "display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "display(Markdown(f\"Converting {model_name} ...\"))\n",
    "\n",
    "! $convert_command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_output = %sx omz_info_dumper --name $model_name\n",
    "model_info = json.loads(model_info_output.get_nlstr())\n",
    "\n",
    "if len(model_info) > 1:\n",
    "    NotebookAlert(f\"\"\"\n",
    "    There are multiple IR files for the {model_name} model. The first model in the\n",
    "    omz_info_dumper output will be used for benchmarking. Change\n",
    "    `selected_model_info` in the cell below to select a different model from the list warning\n",
    "    \"\"\")\n",
    "\n",
    "model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "selected_model_info = model_info[0]\n",
    "model_path = (\n",
    "    base_model_dir\n",
    "    / Path(selected_model_info['subdirectory'])\n",
    "    / Path(f\"{precision}/{selected_model_info['name']}.xml\")\n",
    ")\n",
    "\n",
    "print(f\"{model_path} exists: {model_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Benchmark tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_command = f\"benchmark_app -m {model_path} -t 15\"\n",
    "\n",
    "display(Markdown(f\"Benchmark command: `{benchmark_command}`\"))\n",
    "display(Markdown(f\"Benchmarking {model_name} on CPU with async inference for 15 seconds...\"))\n",
    "\n",
    "! $benchmark_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark with Different Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_xml, device=\"CPU\", seconds=60, api=\"async\", batch=1):\n",
    "    ie = Core()\n",
    "    model_path = Path(model_xml)\n",
    "\n",
    "    if \"GPU\" in device and \"GPU\" not in ie.available_devices:\n",
    "        DeviceNotFoundAlert(\"GPU\")\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch}\"\n",
    "        display(Markdown(f\"**Benchmark {model_path.name} with {device} for {seconds} with {api} inference**\"))\n",
    "        display(Markdown(f\"Benchmark command : `{benchmark_command}`\"))\n",
    "\n",
    "        benchmark_output = %sx $benchmark_command\n",
    "        print(\"command ended\")\n",
    "        benchmark_result = [line for line in benchmark_output\n",
    "                            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "\n",
    "        print(\"\\n\".join(benchmark_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "# Show devices available for OpenVINO Runtime\n",
    "for device in ie.available_devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"CPU\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"AUTO\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"GPU\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_path, device=\"MULTI:CPU,GPU\", seconds=15, api=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0799a591d602a7578f1ccf5f8e7829399f6cceabacecc6a7299b85400bce773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
