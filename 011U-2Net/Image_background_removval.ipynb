{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Background removal with $U^2$-Net and OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the PyTorch Library and $U^2$-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML, FileLink, display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from utils.models.u2net import U2NET, U2NETP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"data\"\n",
    "model_config = namedtuple(\"ModelConfig\", [\"name\", \"url\", \"model\", \"model_args\"])\n",
    "\n",
    "u2net_lite = model_config(\n",
    "    name=\"u2net_lite\",\n",
    "    url=\"https://drive.google.com/uc?id=1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy\",\n",
    "    model=U2NETP,\n",
    "    model_args=(),\n",
    ")\n",
    "u2net = model_config(\n",
    "    name=\"u2net\",\n",
    "    url=\"https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\",\n",
    "    model=U2NET,\n",
    "    model_args=(3, 1),\n",
    ")\n",
    "u2net_human_seg = model_config(\n",
    "    name=\"u2net_human_seg\",\n",
    "    url=\"https://drive.google.com/uc?id=1-Yg0cxgrNhHP-016FPdp902BR-kSsA4P\",\n",
    "    model=U2NET,\n",
    "    model_args=(3, 1),\n",
    ")\n",
    "\n",
    "# Set u2net_model to one of the three configurations listed above.\n",
    "u2net_model = u2net_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filenames of the downloaded and converted models.\n",
    "MODEL_DIR = \"model\"\n",
    "model_path = Path(MODEL_DIR) / u2net_model.name / Path(u2net_model.name).with_suffix(\".pth\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### debug\n",
    "print(model_path)\n",
    "print(onnx_path)\n",
    "print(ir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the $U^2$-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_path.exists():\n",
    "    import gdown\n",
    "\n",
    "    os.makedirs(name=model_path.parent, exist_ok=True)\n",
    "    print(\"Start downloadig model weights file...\")\n",
    "    with open(model_path, \"wb\") as model_file:\n",
    "        gdown.download(url=u2net_model.url, output=model_file)\n",
    "        print(f\"Model weights have been downloaded to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "net = u2net_model.model(*u2net_model.model_args)\n",
    "net.eval()\n",
    "\n",
    "# Load the weights.\n",
    "print(f\"Loading model weights from: '{model_path}'\")\n",
    "net.load_state_dict(state_dict=torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Save the model if it does not exist yet.\n",
    "if not model_path.exists():\n",
    "    print(\"\\nSaving the model\")\n",
    "    torch.save(obj=net.state_dict(), f=str(model_path))\n",
    "    print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convet PyTorch $U^2$-Net model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, 512, 512)\n",
    "    torch.onnx.export(model=net, args=dummy_input, f=onnx_path, opset_version=11)\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ONNX model to OpenVINO IR Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer\n",
    "# Set log_level to CRITICAL to suppress warnings that can be ignored for this demo\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"model/u2net_lite/u2net_lite.onnx\"\n",
    "                 --input_shape \"[1,3,512,512]\"\n",
    "                 --mean_values=\"[123.675,116.28,103.53]\"\n",
    "                 --scale_values=\"[58.395,57.12,57.375]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"model/u2net_lite\"\n",
    "                 --log_level \"CRITICAL\"\n",
    "                 \"\"\"\n",
    "\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO\")\n",
    "print(mo_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX mdoel to IR... This may take a few minutes\")\n",
    "    output = subprocess.check_output(mo_command, shell=True)\n",
    "    print (output.decode('utf-8'))\n",
    "    # mo_result = %sx $mo_command\n",
    "    # print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Pre-Process input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = Path(IMAGE_DIR) / \"coco_dog.jpg\"\n",
    "\n",
    "image = cv2.cvtColor(\n",
    "    src=cv2.imread(filename=str(IMAGE_PATH)),\n",
    "    code=cv2.COLOR_BGR2RGB\n",
    ")   \n",
    "\n",
    "resized_image = cv2.resize(image, (512, 512))\n",
    "\n",
    "# Convert the image shape to a shape and a data type expected by the network\n",
    "# for OpenVINO IR model: (1, 3, 512, 512)\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do inference on OpenVINO IR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network to OpenVINO Runtime.\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=ir_path)\n",
    "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get the names of input and output layers\n",
    "input_layer_ir = compiled_model_ir.input(0)\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# De inference on the input image\n",
    "start_time = time.perf_counter()\n",
    "result = compiled_model_ir([input_image])[output_layer_ir]\n",
    "ent_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resize the network result to the image shpae and round the values\n",
    "to 0 (background) and 1 (foreground).\n",
    "The network result has (1, 1, 512, 512) shape. The `np.squeeze` function converts this to (512, 512)\n",
    "\"\"\"\n",
    "\n",
    "resized_result = np.rint(\n",
    "    cv2.resize(src=np.squeeze(result), dsize=(image.shape[1], image.shape[0]))\n",
    ").astype(np.uint8)\n",
    "\n",
    "# Create a copy of the image and set all background values to 255 (white).\n",
    "bg_removed_result = image.copy()\n",
    "bg_removed_result[resized_result == 0] = 255\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 7))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(resized_result, camp=\"gray\")\n",
    "ax[2].imshow(bg_removed_result)\n",
    "for a in ax:\n",
    "    a.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a New Background Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKGROUND_FILE = \"./data/wall.jpg\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "os.makedirs(name=OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "background_image = cv2.cvtColor(src=cv2.imread(BACKGROUND_FILE), code=cv2.COLOR_BGR2RGB)\n",
    "background_resize = cv2.resize(background_image, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Set all the foreground pixels from the result to 0\n",
    "# in the background image and the image with the background removed.\n",
    "background_image[resized_result == 1] = 0\n",
    "new_image = background_image + bg_removed_result\n",
    "\n",
    "# Save the generated image.\n",
    "new_image_path = Path(f\"{OUTPUT_DIR}/{IMAGE_PATH.stem}-{Path(BACKGROUND_FILE).stem}.jpg\")\n",
    "cv2.imwrite(filename=str(new_image_path), img=cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Display the original image and the image with the new background side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 7))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(new_image)\n",
    "for a in ax:\n",
    "    a.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Create a link to download the image\n",
    "image_link = FileLink(new_image_path)\n",
    "image_link.html_link_str = \"<a href='%s' download>%s</a>\"\n",
    "display(\n",
    "    HTML(\n",
    "        f\"The generated image <code>{new_image_path.name}</code> is saved in\"\n",
    "        f\"the directory <code>{new_image_path.parent}</code>. You can also\"\n",
    "        f\"{image_link._repr_html_()}\"\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0799a591d602a7578f1ccf5f8e7829399f6cceabacecc6a7299b85400bce773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
