{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PyTorch model to ONNX and OpenVINO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "from fastseg import MobileV3Large\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from notebook_utils import CityScapesSegmentation, segmentation_map_to_image, viz_result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 1024  # Suggested values: 2048, 1024 or 512. The minimum width is 512.\n",
    "# Set IMAGE_HEIGHT manually for custom input sizes. Minimum height is 512\n",
    "IMAGE_HEIGHT = 1024 if IMAGE_WIDTH == 2048 else 512\n",
    "DIRECTORY_NAME = \"model\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/fastseg{IMAGE_WIDTH}\"\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Fastseg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading the Fastseg model (if it has not been downloaded before)....\")\n",
    "model = MobileV3Large.from_pretrained().eval()\n",
    "print(\"Loaded PyTorch Fastseg model\")\n",
    "\n",
    "# Save the model\n",
    "model_path.parent.mkdir(exist_ok=True)\n",
    "torch.save(model.state_dict(), str(model_path))\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX model Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PyTorch model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "    # For the Fastseg model, setting do_constant_folding to False is required\n",
    "    # for PyTorch>1.5.1\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1, 3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
    "                 --mean_values=\"[123.675, 116.28, 103.53]\"\n",
    "                 --scale_values=\"[58.395, 57.12, 57.375]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "\"\"\"\n",
    "\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to openVINO\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    image = image.astype(np.float32)\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    image /= 255.0\n",
    "    image -= mean\n",
    "    image /= std\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = \"data/street.jpg\"\n",
    "image = cv2.cvtColor(cv2.imread(image_filename), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "resized_image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "normalized_image = normalize(resized_image)\n",
    "\n",
    "# Convert the resized images to network input shape\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\n",
    "normalized_input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the OpenVINO IR Network and Run Inference on the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model_onnx = ie.read_model(model=onnx_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "res_onnx = compiled_model_onnx([normalized_input_image])[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert network result to segmentation map and display the result\n",
    "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1)).astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result_mask_onnx, CityScapesSegmentation.get_colormap()),\n",
    "    resize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR Model in Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model_ir = ie.read_model(model=ir_path)\n",
    "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(compiled_model_ir.inputs))\n",
    "output_layer_ir = next(iter(compiled_model_ir.outputs))\n",
    "\n",
    "res_ir = compiled_model_ir([input_image])[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1)).astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result=result_mask_ir, colormap=CityScapesSegmentation.get_colormap()),\n",
    "    resize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_torch = model(torch.as_tensor(normalized_input_image).float())\n",
    "\n",
    "result_mask_torch = torch.argmax(result_torch, dim=1).squeeze(0).numpy().astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result=result_mask_torch, colormap=CityScapesSegmentation.get_colormap()),\n",
    "    resize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 20\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images): # \"_\" -> ループの中で変数が使われていないことを表す記号\n",
    "    compiled_model_onnx([normalized_input_image])\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(\n",
    "    f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.3f}\"\n",
    "    f\"seconds per image, FPS: {num_images/time_onnx:.2f}\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_ir([input_image])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.3f}\"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        model(torch.as_tensor(input_image).float())\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "print(\n",
    "    f\"PyTorch model on CPU: {time_torch/num_images:.3f} seconds per image,\"\n",
    "    f\"FPS: {num_images/time_torch:.2f}\"\n",
    ")\n",
    "\n",
    "if \"GPU\" in ie.available_devices:\n",
    "    compiled_model_onnx_gpu = ie.compile_model(model=model_onnx, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_onnx_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_onnx_gpu = end - start\n",
    "\n",
    "    print(\n",
    "        f\"ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.3f}\"\n",
    "        f\"seconds per image {num_images/time_onnx_gpu:.2f}\"\n",
    "    )\n",
    "\n",
    "    compiled_model_ir_gpu = ie.compile_model(model=model_ir, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_ir_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_ir_gpu = end - start\n",
    "\n",
    "    print(\n",
    "        f\"IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.3f}\"\n",
    "        f\"seconds per image, FPS: {num_images/time_ir_gpu:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show device information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ie.available_devices\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device_name=device, name=\"FULL_DEVIDCE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモ\n",
    "\n",
    "処理時間の計測方法\n",
    "- perf_counter()・・・パフォーマンスカウンターを取得する\n",
    "- process_time()・・・CPUの処理時間を求める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0799a591d602a7578f1ccf5f8e7829399f6cceabacecc6a7299b85400bce773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
