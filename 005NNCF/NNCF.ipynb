{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Quantization of PyTorch models with NNCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\Hostx86\\x64 to PATH\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "On Windows, this script adds the directory that contains cl.exe to the PATH TO enable PyTorch to find the\n",
    "required C++ tools. This code assumes that Visual Studio 2019 is installed in the default directory.\n",
    "If you have a different C++ compiler, please add the correct path to os.environ[\"PATH\"] directory.\n",
    "Adding the path to os.environ[\"LIB\"] is not always required - it depends on the system's configuration\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    import distutils.command.build_ext\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    VS_INSTALL_DIR = r\"C:/Program Files (x86)/Microsoft Visual Studio\"\n",
    "    cl_paths = sorted(list(Path(VS_INSTALL_DIR).glob(\"**/Hostx86/x64/cl.exe\")))\n",
    "    if len(cl_paths) == 0:\n",
    "        raise ValueError(\n",
    "            \"\"\"\n",
    "            Cannot find Visual Studio. This notebook requires C++ If you installed a C++ compiler,\n",
    "            please add the directory that contains cl.exe to `os.environ['PATH']`\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        # If multiple versions of MSVC are installed, get the most recent version\n",
    "        cl_path = cl_paths[-1]\n",
    "        vs_dir = str(cl_path.parent)\n",
    "        os.environ[\"PATH\"] += f\"{os.pathsep}{vs_dir}\"\n",
    "        # Code for finding the library dirs from\n",
    "        # https://stackoverflow.com/questions/47423246/get-pythons-lib-path\n",
    "        d = distutils.core.Distribution()\n",
    "        b = distutils.command.build_ext.build_ext(d)\n",
    "        b.finalize_options()\n",
    "        os.environ[\"LIB\"] = os.pathsep.join(b.library_dirs)\n",
    "        print(f\"Added {vs_dir} to PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hokuto\\AppData\\Roaming\\Python\\Python39\\site-packages\\nncf\\torch\\__init__.py:23: UserWarning: NNCF provides best results with torch==1.9.1, while current torch version is 1.8.1+cpu - consider switching to torch==1.9.1\n",
      "  warnings.warn(\"NNCF provides best results with torch=={bkc}, \"\n",
      "c:\\Users\\hokuto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\cpp_extension.py:304: UserWarning: Error checking compiler version for cl: 'utf-8' codec can't decode byte 0x8e in position 130: invalid start byte\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "C:\\Users\\hokuto\\AppData\\Roaming\\Python\\Python39\\site-packages\\nncf\\torch\\dynamic_graph\\patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n",
      "c:\\Users\\hokuto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 079eae91b01d7666471c9e01dadd031e2c2a00f2- is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from nncf import NNCFConfig\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "\n",
    "set_log_level(logging.ERROR)\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from openvino.runtime import Core\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205d335c138e4941bb64b657800c78aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model\\resnet50_fp32.pth:   0%|          | 0.00/91.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/hokuto/Desktop/practice/LearnOpenVino/005NNCF/model/resnet50_fp32.pth')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "MODEL_DIR = Path('model')\n",
    "OUTPUT_DIR = Path('output')\n",
    "BASE_MODEL_NAME = 'resnet50'\n",
    "IMAGE_SIZE = [64, 64]\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVino IR models will be stored\n",
    "fp32_checkpoint_filename = Path(BASE_MODEL_NAME + \"_fp32\").with_suffix('.pth')\n",
    "fp32_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix('.onnx')\n",
    "fp32_ir_path = fp32_onnx_path.with_suffix('.xml')\n",
    "int8_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_int8\")).with_suffix('.onnx')\n",
    "int8_ir_path = int8_onnx_path.with_suffix('.xml')\n",
    "\n",
    "fp32_pth_url = 'https://storage.openvinotoolkit.org/repositories/nncf/openvino_notebook_ckpts/304_resnet50_fp32.pth'\n",
    "download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Prepare Tiny ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tiny_imagenet_200(\n",
    "    output_dir: Path,\n",
    "    url: str = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n",
    "    tarname: str = \"tiny-imagenet-200.zip\"):\n",
    "\n",
    "    archive_path = output_dir / tarname\n",
    "    download_file(url, directory=output_dir, filename=tarname)\n",
    "    zip_ref = zipfile.ZipFile(archive_path, 'r')\n",
    "    zip_ref.extractall(path=output_dir)\n",
    "    zip_ref.close()\n",
    "    print(f\"Successfully downloaded and extracted dataset to: {output_dir}\")\n",
    "\n",
    "def create_validation_dir(dataset_dir: Path):\n",
    "    VALID_DIR = dataset_dir / 'val'\n",
    "    val_img_dir = VALID_DIR / 'images'\n",
    "\n",
    "    fp = open(VALID_DIR / 'val_annotations.txt', 'r')\n",
    "    data = fp.readlines()\n",
    "\n",
    "    val_img_dict = {}\n",
    "    for line in data:\n",
    "        words = line.split(\"\\t\")\n",
    "        val_img_dict[words[0]] = words[1]\n",
    "    fp.close()\n",
    "\n",
    "    for img, folder in val_img_dict.items():\n",
    "        newpath = val_img_dir / folder\n",
    "        if not newpath.exists():\n",
    "            os.makedirs(newpath)\n",
    "        if (val_img_dir / img).exists():\n",
    "            os.remove(val_img_dir / img, newpath / img)\n",
    "\n",
    "DATASET_DIR = OUTPUT_DIR / 'tiny-imagenet-200'\n",
    "if not DATASET_DIR.exists():\n",
    "    download_tiny_imagenet_200(OUTPUT_DIR)\n",
    "    create_validation_dir(DATASET_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Helper classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and curent value\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, fmt: str = \":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    \"\"\"Displays the progress of validation process\"\"\"\n",
    "\n",
    "    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = \"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def display(self, batch: int):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches: int):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int] = (1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function\n",
    "\n",
    "ここから先はcudaが搭載されているPCで実装したほうがよさそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module):\n",
    "    \"\"\"Compute the metrics using data from val_loader for the model\"\"\"\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, top1, top5], prefix=\"Test:\")\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 10\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "        \n",
    "        print(f\"* Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\")\n",
    "\n",
    "    return top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and load original uncompressed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_path: Path):\n",
    "    \"\"\"Creates the ResNet-50 model and loadas the pretrained weights\"\"\"\n",
    "    model = models.resnet50()\n",
    "    # update the last FC layer fro Tiny ImageNet number of classes\n",
    "    NUM_CLASSES = 200\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=NUM_CLASSES, bias=True)\n",
    "    model.to(device)\n",
    "    if model_path.exists():\n",
    "        checkpoint = torch.load(str(model_path), map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"], strict=True)\n",
    "    else:\n",
    "        raise RuntimeError(\"There is no checkpoint to load\")\n",
    "    return model\n",
    "\n",
    "model = create_model(MODEL_DIR / fp32_checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size: int=128):\n",
    "    \"\"\"Creates train dataloader that is used for quantization initialization\"\"\"\n",
    "    train_dir = DATASET_DIR / \"train\"\n",
    "    val_dir = DATASET_DIR / \"val\" / \"images\"\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        train_dir,\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(IMAGE_SIZE),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        val_dir,\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(IMAGE_SIZE), \n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        sampler=None,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = create_dataloaders()\n",
    "\n",
    "# debug\n",
    "print(train_loader)\n",
    "print(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = validate(val_loader, model)\n",
    "print(f\"Test accuracy of FP32 model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, *IMAGE_SIZE).to(device)\n",
    "torch.onnx.export(model, dummy_input, fp32_onnx_path, opset_version=10)\n",
    "print(f\"FP32 ONNX model was exported to {fp32_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and initialize quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 3, *IMAGE_SIZE]},\n",
    "    \"log_dir\": str(OUTPUT_DIR),\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "        \"initializer\": {\n",
    "            \"range\": {\"num_init_samples\": 15000},\n",
    "            \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": 4000},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "\n",
    "# debug\n",
    "print(nncf_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc688668df90e2da2c6fe127a4fae0fc63e05cce4be11dcfea3b7cd731a68cc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
