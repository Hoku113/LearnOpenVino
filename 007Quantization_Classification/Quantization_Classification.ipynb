{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of Image Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from addict import Dict\n",
    "from openvino.tools.pot.api import DataLoader, Metric\n",
    "from openvino.tools.pot.engines.ie_engine import IEEngine\n",
    "from openvino.tools.pot.graph import load_model, save_model\n",
    "from openvino.tools.pot.graph.model_utils import compress_model_weights\n",
    "from openvino.tools.pot.pipeline.initializer import create_pipeline\n",
    "from openvino.runtime import Core\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data and model directories\n",
    "DATA_DIR = 'data'\n",
    "MODEL_DIR = 'model'\n",
    "\n",
    "try:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "except OSError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x1_0\",  pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "onnx_model_path = Path(MODEL_DIR)/ 'mobilenet_v2.onnx'\n",
    "ir_model_xml = onnx_model_path.with_suffix('.xml')\n",
    "ir_model_bin = onnx_model_path.with_suffix('.bin')\n",
    "\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "# Run OpenVINO Model Optimization tool to convert ONNX to OpenVINO IR\n",
    "!mo --framework=onnx --data_type=FP16 --input_shape=[1,3,32,32] -m $onnx_model_path  --output_dir $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine DataLoader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "dataset = CIFAR10(root=DATA_DIR, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize config and dataset.\n",
    "        :param config: created config with DATA_DIR path.\n",
    "        \"\"\"\n",
    "        if not isinstance(config, Dict):\n",
    "            config = Dict(config)\n",
    "        super().__init__(config)\n",
    "        self.indexes, self.pictures, self.labels = self.load_data(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return one sample of index, label and picture.\n",
    "        :param index: index of the taken sample.\n",
    "        \"\"\"\n",
    "        if index >= len(self):\n",
    "            raise IndexError\n",
    "\n",
    "        return (self.indexes[index], self.labels[index]), self.pictures[index].numpy()\n",
    "\n",
    "    def load_data(self, dataset):\n",
    "        \"\"\"\n",
    "        Load dataset in needed format. \n",
    "        :param dataset:  downloaded dataset.\n",
    "        \"\"\"\n",
    "        pictures, labels, indexes = [], [], []\n",
    "        \n",
    "        for idx, sample in enumerate(dataset):\n",
    "            pictures.append(sample[0])\n",
    "            labels.append(sample[1])\n",
    "            indexes.append(idx)\n",
    "\n",
    "        return indexes, pictures, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Accuracy Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of classification accuracy metric\n",
    "\n",
    "class Accuracy(Metric):\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self, top_k=1):\n",
    "        super().__init__()\n",
    "        self._top_k = top_k\n",
    "        self._name = f'accuracy@top{self._top_k}'\n",
    "        self._matches = []\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        # Returns accuracy metric value for the last model output.\n",
    "        return {self._name: self._matches[-1]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        # Returns accuracy metric value for all model outputs\n",
    "        return {self._name: np.ravel(self._matches).mean()}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        Updates prediction matches\n",
    "        :param output: model output\n",
    "        :param target; annotations\n",
    "        \"\"\"\n",
    "\n",
    "        if len(output) > 1:\n",
    "            raise Exception('The accuracy metric cannot be calculated for a model with multiple outputs')\n",
    "\n",
    "        if isinstance(target, dict):\n",
    "            target = list(target.values())\n",
    "        predictions = np.argsort(output[0], axis=1)[:, -self._top_k:]\n",
    "        match = [float(t in predictions[i]) for i, t in enumerate(target)]\n",
    "\n",
    "        self._matches.append(match)\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets collected matces\n",
    "        self._matches = []\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name:{attribute_name:value}}\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse' 'type': metric type\n",
    "        \"\"\"\n",
    "\n",
    "        return {self._name: {\"direction\": \"higher-better\", \"type\": \"accuracy\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Quantization Pipeline and compare the accuracy of the original and quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = Dict({\n",
    "    'model_name': 'mobilenet_v2',\n",
    "    'model': ir_model_xml,\n",
    "    'weights': ir_model_bin\n",
    "})\n",
    "\n",
    "engine_config = Dict({\n",
    "    'device': 'CPU',\n",
    "    'start_requests_number': 2,\n",
    "    'eval_requests_number': 2\n",
    "})\n",
    "\n",
    "dataset_config = {\n",
    "    'data_source': DATA_DIR\n",
    "}\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        'name': 'DefaultQuantization',\n",
    "        'params': {\n",
    "            'target_device': 'CPU',\n",
    "            'preset': 'performance',\n",
    "            'start_subset_size': 300\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "model = load_model(model_config)\n",
    "\n",
    "data_loader = CifarDataLoader(dataset_config)\n",
    "\n",
    "metric = Accuracy(top_k=1)\n",
    "\n",
    "engine = IEEngine(engine_config, data_loader, metric)\n",
    "\n",
    "pipeline = create_pipeline(algorithms, engine)\n",
    "\n",
    "compressed_model = pipeline.run(model)\n",
    "\n",
    "compress_model_weights(compressed_model)\n",
    "\n",
    "compressed_model_paths = save_model(model=compressed_model, save_path=MODEL_DIR, model_name=\"quantized_mobilenet_v2\")\n",
    "\n",
    "#check\n",
    "print(compressed_model_paths)\n",
    "\n",
    "compressed_model_xml = compressed_model_paths[0][\"model\"]\n",
    "compressed_model_bin = Path(compressed_model_paths[0][\"model\"]).with_suffix(\".bin\")\n",
    "\n",
    "# check\n",
    "print(compressed_model_xml)\n",
    "\n",
    "metric_results = pipeline.evaluate(model)\n",
    "\n",
    "# check value\n",
    "print(metric_results)\n",
    "\n",
    "if metric_results:\n",
    "    for name, value in metric_results.items():\n",
    "        print(f\"Accuracy of the original model: {name}: {value}\")\n",
    "\n",
    "metric_results = pipeline.evaluate(compressed_model)\n",
    "if metric_results:\n",
    "    for name, value in metric_results.items():\n",
    "        print(f\"Accuracy of the optimized model: {name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance of the Original and Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP16 model(IR)\n",
    "!benchmark_app -m $ir_model_xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference INT8 model(IR)\n",
    "!benchmark_app -m $compressed_model_xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results on four pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "# read original model\n",
    "float_model = ie.read_model(\n",
    "    model=ir_model_xml, weights=ir_model_bin\n",
    ")\n",
    "\n",
    "float_compiled_model = ie.compile_model(model=float_model, device_name=\"CPU\")\n",
    "\n",
    "# read quantized model\n",
    "quantized_model = ie.read_model(\n",
    "    model=compressed_model_xml, weights=compressed_model_bin\n",
    ")\n",
    "\n",
    "quantized_compiled_model = ie.compile_model(model=quantized_model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all possible labels from CIFAR10\n",
    "label_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "all_pictures = []\n",
    "all_labels = []\n",
    "\n",
    "# get all pictures and their labels\n",
    "for i, batch in enumerate(data_loader):\n",
    "    all_pictures.append(batch[1])\n",
    "    all_labels.append(batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "def plot_pictures(indexes: list, all_pictures=all_pictures, all_labels=all_labels):\n",
    "    \"\"\"\n",
    "    Plot 4 pictures\n",
    "    :param indexes: a list of indexes of images to be displayed\n",
    "    :param all_labels: labels with pictures\n",
    "    \"\"\"\n",
    "\n",
    "    images, labels = [], []\n",
    "    num_pics = len(indexes)\n",
    "    assert num_pics == 4, f'No enough indexes for pictures to be displayed, got {num_pics}'\n",
    "    for idx in indexes:\n",
    "        assert idx < 10000, 'Cannot get such index, there are only 10000'\n",
    "        pic = np.rollaxis(all_pictures[idx].squeeze(), 0, 3)\n",
    "        images.append(pic)\n",
    "\n",
    "        labels.append(label_names[all_labels[idx]])\n",
    "\n",
    "    f, axarr = plt.subplots(1, 4)\n",
    "    axarr[0].imshow(images[0])\n",
    "    axarr[0].set_title(labels[0])\n",
    "\n",
    "    axarr[1].imshow(images[1])\n",
    "    axarr[1].set_title(labels[1])\n",
    "\n",
    "    axarr[2].imshow(images[2])\n",
    "    axarr[2].set_title(labels[2])\n",
    "    \n",
    "    axarr[3].imshow(images[3])\n",
    "    axarr[3].set_title(labels[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_pictures(model, indexes: list, all_pictures=all_pictures):\n",
    "    \"\"\"\n",
    "    Inference model on a few pictures\n",
    "    :param net: model on which do inference\n",
    "    :param indexes: list of indexes\n",
    "    \"\"\"\n",
    "\n",
    "    output_key = model.output(0)\n",
    "    predicted_labels = []\n",
    "    for idx in indexes:\n",
    "        assert idx < 10000, 'Cannot get such index, there are only 10000'\n",
    "        result = model([all_pictures[idx][None,]])[output_key]\n",
    "        result = label_names[np.argmax(result[0])]\n",
    "        predicted_labels.append(result)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_infer = [7, 12, 15, 20] # to plot specify 4 indexes\n",
    "\n",
    "plot_pictures(indexes_to_infer)\n",
    "\n",
    "results_float = infer_on_pictures(float_compiled_model, indexes_to_infer)\n",
    "results_quantized = infer_on_pictures(quantized_compiled_model, indexes_to_infer)\n",
    "\n",
    "print(f\"Labels for picture from float model: {results_float}\")\n",
    "print(f\"Labels for picture from quantized model: {results_quantized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.openvino-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c72c11d2d2d54e76eb9b9746a9dfd04f5bf9e7b246a1d2f2ce53b6ae392c21ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
